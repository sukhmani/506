{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "xGWLVal_rx4B",
      "metadata": {
        "id": "xGWLVal_rx4B"
      },
      "source": [
        "Here's a step-by-step in-class assignment designed to introduce CS506 students to deep learning using Python, Keras, and scikit-learn. This assignment aims for hands-on experience and encourages experimentation.\n",
        "\n",
        "## Deep Learning Fundamentals: A Hands-On Keras & scikit-learn Workshop\n",
        "\n",
        "**Objective:** This assignment will guide you through the fundamental steps of building, training, and evaluating deep learning models using Python's Keras and scikit-learn libraries. You will learn to load data, create simple neural networks, experiment with optimizers, visualize results, and understand data splitting techniques.\n",
        "\n",
        "**Estimated Time:** 30 - 45 Minutes (can be adapted for longer or shorter sessions)\n",
        "\n",
        "**Materials:**\n",
        "\n",
        "* Jupyter Notebook environment (or Google Colab)\n",
        "\n",
        "* Python 3 installed with `tensorflow` (which includes Keras), `scikit-learn`, `matplotlib`, and `pandas`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FvXAdOi3rx4D",
      "metadata": {
        "id": "FvXAdOi3rx4D"
      },
      "source": [
        "### Part 1: Setting the Stage - Libraries and Data Loading\n",
        "\n",
        "**1.1 Load Essential Python Libraries**\n",
        "\n",
        "* **Task:** Begin by importing all the necessary libraries for this assignment.\n",
        "\n",
        "* **Instructions:** In a new code cell, add the following import statements. Explain briefly what each library is used for in the context of deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "load_libraries",
      "metadata": {
        "id": "load_libraries"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-23 02:54:35.238209: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-05-23 02:54:35.500353: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-05-23 02:54:35.701423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747968876.035217    3936 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747968876.154754    3936 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1747968876.443428    3936 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1747968876.443463    3936 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1747968876.443466    3936 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1747968876.443469    3936 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-05-23 02:54:36.475332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Keras for building neural networks\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop # We'll experiment with these\n",
        "\n",
        "# Scikit-learn for data splitting and preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris, load_wine, load_breast_cancer # We'll choose one\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0d966910",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 3\n",
            "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']...\n",
            "Target names: ['setosa' 'versicolor' 'virginica']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X  = data.data # data\n",
        "y =data.target # label\n",
        "\n",
        "# Display shape of features and labels\n",
        "print(f\"Number of classes: {np.unique(y).shape[0]}\")\n",
        "print(f\"Feature names: {data.feature_names[:5]}...\")  \n",
        "print(f\"Target names: {data.target_names}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-30Oxtt_rx4D",
      "metadata": {
        "id": "-30Oxtt_rx4D"
      },
      "source": [
        "* **Discussion Point:** Why do we typically import these libraries at the beginning of our script?\n",
        "\n",
        "**1.2 Load a Dataset from Keras or scikit-learn**\n",
        "\n",
        "* **Task:** Choose one of the pre-loaded datasets from `sklearn.datasets` to work with. For simplicity, we'll start with a classification problem.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Choose *one* of the following datasets: `load_iris()`, `load_wine()`, or `load_breast_cancer()`.\n",
        "\n",
        "    * Load the chosen dataset and assign its `data` to `X` (features) and `target` to `y` (labels).\n",
        "\n",
        "    * Print the shape of `X` and `y` to understand the dimensions of your data.\n",
        "\n",
        "    * Briefly describe the dataset you've chosen (e.g., number of features, number of classes)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t2ljrxW1rx4E",
      "metadata": {
        "id": "t2ljrxW1rx4E"
      },
      "source": [
        "* **Self-Correction/Extension:** If you're feeling adventurous, try loading a dataset from Keras directly (e.g., `keras.datasets.mnist.load_data()`). Be aware that Keras datasets often come pre-split.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67FBHsearx4E",
      "metadata": {
        "id": "67FBHsearx4E"
      },
      "source": [
        "### Part 2: Data Preprocessing and Splitting\n",
        "\n",
        "**2.1 Data Splitting (Train, Validation, Test)**\n",
        "\n",
        "* **Task:** Split your dataset into training, validation, and testing sets. This is crucial for evaluating your model's generalization performance.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Use `train_test_split` from `sklearn.model_selection`.\n",
        "\n",
        "    * First, split `X` and `y` into `X_train`, `X_test`, `y_train`, `y_test` with a `test_size` of 20-30% and `random_state` for reproducibility.\n",
        "\n",
        "    * Then, further split `X_train` and `y_train` into `X_train_split`, `X_val`, `y_train_split`, `y_val` (e.g., 20% of the training data for validation).\n",
        "\n",
        "    * Print the shapes of all resulting arrays (`X_train_split`, `X_val`, `X_test`, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "60117302",
      "metadata": {},
      "outputs": [],
      "source": [
        "h = .02  # step size in the mesh\n",
        "\n",
        "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
        "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
        "         \"Naive Bayes\", \"QDA\"]\n",
        "\n",
        "\n",
        "           "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "11ac0b05",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "  stratify=y,\n",
        "  test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "data_splitting",
      "metadata": {
        "id": "data_splitting"
      },
      "outputs": [],
      "source": [
        "# Split into training and testing sets\n",
        "\n",
        "# Using stratify=y ensures that the proportion of target values is the same in both the training and test sets.\n",
        "\n",
        "# Split training set into training and validation sets\n",
        "\n",
        "\n",
        "# display train and test dataset shape\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "  stratify=y,\n",
        "  test_size=0.25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UvOtAXPbrx4E",
      "metadata": {
        "id": "UvOtAXPbrx4E"
      },
      "source": [
        "* **Discussion Point:** Why is it important to have separate training, validation, and test sets? What happens if we only use a train-test split?\n",
        "\n",
        "**2.2 Feature Scaling**\n",
        "\n",
        "* **Task:** Standardize your features. This is a common and often crucial preprocessing step for neural networks, as it helps optimizers converge faster and more effectively.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Initialize a `StandardScaler` from `sklearn.preprocessing`.\n",
        "\n",
        "    * Fit the scaler *only* on your `X_train` data.\n",
        "\n",
        "    * Transform `X_train`, `X_val`, and `X_test` using the fitted scaler.\n",
        "\n",
        "    * Print the mean and standard deviation of a few features in `X_train` *after* scaling to verify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "feature_scaling",
      "metadata": {
        "id": "feature_scaling"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean of the first 5 features (scaled): [-9.19899078e-16 -2.22044605e-16  1.50673125e-16 -3.81639165e-17]\n",
            "Standard deviation of the first 5 features (scaled): [1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "# scale the features\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train = scaler.fit_transform(X_train)\n",
        "\n",
        "# Calculate and print the mean of the first 5 features\n",
        "mean = np.mean(train[:, :5], axis=0)\n",
        "print(\"Mean of the first 5 features (scaled):\", mean)\n",
        "\n",
        "# Calculate and print the standard deviation of the first 5 features\n",
        "std = np.std(train[:, :5], axis=0)\n",
        "print(\"Standard deviation of the first 5 features (scaled):\", std)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3lvlGiy1rx4E",
      "metadata": {
        "id": "3lvlGiy1rx4E"
      },
      "source": [
        "* **Discussion Point:** Why do we fit the scaler *only* on the training data and then transform all sets? What would happen if we fitted on the entire dataset before splitting?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mhCLdYW0rx4F",
      "metadata": {
        "id": "mhCLdYW0rx4F"
      },
      "source": [
        "### Part 3: Building and Training Your First Model\n",
        "\n",
        "**3.1 Create a Simple Neural Network Model**\n",
        "\n",
        "* **Task:** Construct a basic feed-forward neural network using Keras's Sequential API.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Define a `Sequential` model.\n",
        "\n",
        "    * Add a few `Dense` layers. The first `Dense` layer needs an `input_shape` argument (based on the number of features in your dataset).\n",
        "\n",
        "    * Use appropriate activation functions (e.g., 'relu' for hidden layers, 'sigmoid' for binary classification output, 'softmax' for multi-class classification output).\n",
        "\n",
        "    * For binary classification (like `load_breast_cancer`), the output layer should have 1 neuron with a 'sigmoid' activation. For multi-class (like `load_iris` or `load_wine`), the output layer should have `num_classes` neurons with 'softmax' activation.\n",
        "\n",
        "    * Print a `model.summary()` to see the architecture and number of parameters.\n",
        "\n",
        "* **Additional Information**\n",
        "    1. Dropout Layer\n",
        "\n",
        "      Purpose: Dropout is a regularization technique used to prevent overfitting. During training, it randomly sets a fraction of input units to 0 at each update step, which helps prevent complex co-adaptations on training data.\n",
        "        \n",
        "        Syntax: `layers.Dropout(rate)`\n",
        "                rate: The fraction of the input units to drop (between 0 and 1). A common value is 0.2 to 0.5.\n",
        "    2. Batch Normalization Layer\n",
        "\n",
        "      Purpose: Batch Normalization normalizes the activations of the previous layer. It helps stabilize and speed up the training process, especially for deeper networks, by reducing internal covariate shift.\n",
        "        \n",
        "        Syntax: `layers.BatchNormalization()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "create_model",
      "metadata": {
        "id": "create_model"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "2025-05-23 03:35:33.326936: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> (772.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m193\u001b[0m (772.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> (772.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m193\u001b[0m (772.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input dimension: 4\n",
            "Number of classes: 3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Determine the number of features for the input layer\n",
        "input_dim = X.shape[1]  # Number of features (columns)\n",
        "\n",
        "# Determine the number of unique classes\n",
        "num_classes = np.unique(y).shape[0]  # Number of unique classes\n",
        "\n",
        "model = Sequential([\n",
        "    layers.Dense(32, activation='relu', input_shape=(input_dim,)),\n",
        "    layers.Dense(1, activation='sigmoid') # For binary classification\n",
        "    # For multi-class classification (e.g., Iris, Wine):\n",
        "    # layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "print(f\"Input dimension: {input_dim}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y-SinAuKrx4F",
      "metadata": {
        "id": "Y-SinAuKrx4F"
      },
      "source": [
        "* **Discussion Point:** What do the 'None' dimensions in `model.summary()` represent? What is the role of activation functions in a neural network?\n",
        "\n",
        "**3.2 Compile the Model**\n",
        "\n",
        "* **Task:** Configure the learning process by specifying the optimizer, loss function, and metrics.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Use `model.compile()`.\n",
        "\n",
        "    * For binary classification, use `loss='binary_crossentropy'`. For multi-class, use `loss='sparse_categorical_crossentropy'` (if your labels are integers) or `loss='categorical_crossentropy'` (if your labels are one-hot encoded).\n",
        "\n",
        "    * Start with the `Adam` optimizer.\n",
        "\n",
        "    * Include `metrics=['accuracy']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "compile_model",
      "metadata": {
        "id": "compile_model"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "tf.keras.layers.Dense(128, activation='relu'),\n",
        "tf.keras.layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "96d03838",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['Accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44e230a6",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_labels' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model.fit(X_train, \u001b[43mtrain_labels\u001b[49m, epochs=\u001b[32m10\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'train_labels' is not defined"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Je1Mo4Z_rx4F",
      "metadata": {
        "id": "Je1Mo4Z_rx4F"
      },
      "source": [
        "* **Discussion Point:** What is the difference between a loss function and a metric? Why do we need an optimizer?\n",
        "\n",
        "**3.3 Train the Model**\n",
        "\n",
        "* **Task:** Train your neural network using the training and validation data.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Use `model.fit()`.\n",
        "\n",
        "    * Pass `X_train_scaled`, `y_train` as training data.\n",
        "\n",
        "    * Pass `X_val_scaled`, `y_val` as validation data (using the `validation_data` argument).\n",
        "\n",
        "    * Set `epochs` (e.g., 20-50) and `batch_size` (e.g., 32).\n",
        "\n",
        "    * Store the training history object in a variable (e.g., `history`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train_model",
      "metadata": {
        "id": "train_model"
      },
      "outputs": [],
      "source": [
        "# train the model with 50 epochs and store output to draw the plots for accuracy and loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RSe2hd9-rx4F",
      "metadata": {
        "id": "RSe2hd9-rx4F"
      },
      "source": [
        "* **Discussion Point:** What are epochs and batch size? How do they affect the training process? What does it mean if the validation loss starts increasing while training loss decreases?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M8ZKGswPrx4F",
      "metadata": {
        "id": "M8ZKGswPrx4F"
      },
      "source": [
        "### Part 4: Visualization and Experimentation\n",
        "\n",
        "**4.1 Visualize Training History**\n",
        "\n",
        "* **Task:** Plot the training and validation loss, and training and validation accuracy over epochs. This helps in understanding model performance and identifying overfitting/underfitting.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Access the `history.history` dictionary.\n",
        "\n",
        "    * Plot 'accuracy' vs. 'val_accuracy' and 'loss' vs. 'val_loss' using `matplotlib.pyplot`.\n",
        "\n",
        "    * Add titles, labels, and legends to your plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "visualize_history",
      "metadata": {
        "id": "visualize_history"
      },
      "outputs": [],
      "source": [
        "# Adjust below code to draw the plots for accuracy and loss\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cQ9lNR6rx4F",
      "metadata": {
        "id": "0cQ9lNR6rx4F"
      },
      "source": [
        "* **Analysis:** Based on the plots, is your model overfitting, underfitting, or performing well? How can you tell?\n",
        "\n",
        "**4.2 Experiment with Different Optimizers**\n",
        "\n",
        "* **Task:** Re-train your model using different optimizers (e.g., `SGD`, `RMSprop`) and compare their performance.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Create a *new* model (or re-initialize your existing one to ensure a clean slate).\n",
        "\n",
        "    * Compile the new model, but this time use `optimizer=SGD()` (with or without learning rate, e.g., `SGD(learning_rate=0.01)`).\n",
        "\n",
        "    * Train the model for the same number of epochs and batch size.\n",
        "\n",
        "    * Repeat the process for `optimizer=RMSprop()`.\n",
        "\n",
        "    * Compare the plots of accuracy and loss for each optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "experiment_optimizers",
      "metadata": {
        "id": "experiment_optimizers"
      },
      "outputs": [],
      "source": [
        "# Experiment with optimizer SGD with learning_rates 0.01, 0.1, 0.2\n",
        "print(\"\\n--- Training with SGD Optimizer ---\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Experiment with RMSprop with learning_rates 0.01, 0.1, 0.2\n",
        "print(\"\\n--- Training with RMSprop Optimizer ---\")\n",
        "\n",
        "\n",
        "# Adjust below code for comparing the models (you'll need to adapt the plotting code from 4.1)\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['val_accuracy'], label='Adam')\n",
        "plt.plot(history_sgd.history['val_accuracy'], label='SGD')\n",
        "plt.plot(history_rmsprop.history['val_accuracy'], label='RMSprop')\n",
        "plt.title('Validation Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['val_loss'], label='Adam')\n",
        "plt.plot(history_sgd.history['val_loss'], label='SGD')\n",
        "plt.plot(history_rmsprop.history['val_loss'], label='RMSprop')\n",
        "plt.title('Validation Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['accuracy'], label='Adam')\n",
        "plt.plot(history_sgd.history['accuracy'], label='SGD')\n",
        "plt.plot(history_rmsprop.history['accuracy'], label='RMSprop')\n",
        "plt.title('Training Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bug_E7RIrx4G",
      "metadata": {
        "id": "Bug_E7RIrx4G"
      },
      "source": [
        "* **Discussion Point:** Which optimizer performed best for your dataset? Why do you think some optimizers perform better than others in certain situations?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BRWamM70rx4G",
      "metadata": {
        "id": "BRWamM70rx4G"
      },
      "source": [
        "### Part 5: Model Evaluation on Test Set\n",
        "\n",
        "**5.1 Evaluate Final Model**\n",
        "\n",
        "* **Task:** Evaluate the performance of your *best-performing* model (from your optimizer experiments) on the unseen test set.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Use `model.evaluate()` (for your best model) on `X_test_scaled` and `y_test`.\n",
        "\n",
        "    * Print the test loss and test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evaluate_model",
      "metadata": {
        "id": "evaluate_model"
      },
      "outputs": [],
      "source": [
        "# Assuming 'model' (trained with Adam) was your best performing model\n",
        "# Or choose model_sgd or model_rmsprop if they performed better\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sBrsHI8Brx4G",
      "metadata": {
        "id": "sBrsHI8Brx4G"
      },
      "source": [
        "* **Discussion Point:** How does the test accuracy compare to the validation accuracy? What does this tell you about your model's generalization?\n",
        "\n",
        "**5.2 Make Predictions and Analyze Metrics**\n",
        "\n",
        "* **Task:** Make predictions on the test set and calculate additional classification metrics.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Use `model.predict()` on `X_test_scaled` to get raw predictions.\n",
        "\n",
        "    * Convert probabilities to binary class predictions (e.g., for sigmoid output, values > 0.5 are class 1, otherwise class 0).\n",
        "\n",
        "    * Calculate and print the `accuracy_score`, `confusion_matrix`, and `classification_report` from `sklearn.metrics`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "analyze_metrics",
      "metadata": {
        "id": "analyze_metrics"
      },
      "outputs": [],
      "source": [
        "y_pred_probs = model.predict(X_test_scaled)\n",
        "# For binary classification with sigmoid output:\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# For multi-class classification with softmax output:\n",
        "# y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "print(\"\\n--- Classification Metrics on Test Set ---\")\n",
        "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jfbu3h81rx4G",
      "metadata": {
        "id": "jfbu3h81rx4G"
      },
      "source": [
        "* **Discussion Point:** What do precision, recall, and F1-score tell you about your model's performance beyond just accuracy? When would each metric be more important?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7StNA6yrx4G",
      "metadata": {
        "id": "d7StNA6yrx4G"
      },
      "source": [
        "### Challenge & Extension Activities (If Time Permits)\n",
        "\n",
        "* **Experiment with Model Architecture:**\n",
        "\n",
        "    * Add more layers or change the number of neurons in existing layers.\n",
        "\n",
        "    * Try different activation functions (e.g., `tanh`, `sigmoid` for hidden layers, though `relu` is often a good default).\n",
        "\n",
        "* **Hyperparameter Tuning:**\n",
        "\n",
        "    * Adjust the `learning_rate` for your optimizers.\n",
        "\n",
        "    * Change the `batch_size` and `epochs`.\n",
        "\n",
        "* **Regularization:** Introduce `Dropout` layers to combat overfitting.\n",
        "\n",
        "* **Different Datasets:** Repeat the process with a different dataset from `sklearn.datasets` (e.g., `load_diabetes` for regression, though you'd need to change the output layer, loss function, and metrics).\n",
        "\n",
        "* **K-Fold Cross-Validation:** Explain how scikit-learn's `KFold` could be used to get a more robust estimate of model performance, especially with smaller datasets. (This is more conceptual for an in-class assignment but good for discussion).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JF3ZHU7wrx4G",
      "metadata": {
        "id": "JF3ZHU7wrx4G"
      },
      "source": [
        "### Deliverables:\n",
        "\n",
        "Students should submit their Jupyter Notebook containing all the code, outputs, and answers to the discussion questions. Encourage them to add comments to their code to explain each step."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
