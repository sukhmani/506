{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here's a step-by-step in-class assignment designed to introduce CS506 students to deep learning using Python, Keras, and scikit-learn. This assignment aims for hands-on experience and encourages experimentation.\n",
        "\n",
        "## Deep Learning Fundamentals: A Hands-On Keras & scikit-learn Workshop\n",
        "\n",
        "**Objective:** This assignment will guide you through the fundamental steps of building, training, and evaluating deep learning models using Python's Keras and scikit-learn libraries. You will learn to load data, create simple neural networks, experiment with optimizers, visualize results, and understand data splitting techniques.\n",
        "\n",
        "**Estimated Time:** 30 - 45 Minutes (can be adapted for longer or shorter sessions)\n",
        "\n",
        "**Materials:**\n",
        "\n",
        "* Jupyter Notebook environment (or Google Colab)\n",
        "\n",
        "* Python 3 installed with `tensorflow` (which includes Keras), `scikit-learn`, `matplotlib`, and `pandas`."
      ],
      "metadata": {
        "id": "xGWLVal_rx4B"
      },
      "id": "xGWLVal_rx4B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Setting the Stage - Libraries and Data Loading\n",
        "\n",
        "**1.1 Load Essential Python Libraries**\n",
        "\n",
        "* **Task:** Begin by importing all the necessary libraries for this assignment.\n",
        "\n",
        "* **Instructions:** In a new code cell, add the following import statements. Explain briefly what each library is used for in the context of deep learning."
      ],
      "metadata": {
        "id": "FvXAdOi3rx4D"
      },
      "id": "FvXAdOi3rx4D"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Keras for building neural networks\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop # We'll experiment with these\n",
        "\n",
        "# Scikit-learn for data splitting and preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import load_iris, load_wine, load_breast_cancer # We'll choose one\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "load_libraries"
      },
      "outputs": [],
      "id": "load_libraries",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Discussion Point:** Why do we typically import these libraries at the beginning of our script?\n",
        "\n",
        "**1.2 Load a Dataset from Keras or scikit-learn**\n",
        "\n",
        "* **Task:** Choose one of the pre-loaded datasets from `sklearn.datasets` to work with. For simplicity, we'll start with a classification problem.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Choose *one* of the following datasets: `load_iris()`, `load_wine()`, or `load_breast_cancer()`.\n",
        "\n",
        "    * Load the chosen dataset and assign its `data` to `X` (features) and `target` to `y` (labels).\n",
        "\n",
        "    * Print the shape of `X` and `y` to understand the dimensions of your data.\n",
        "\n",
        "    * Briefly describe the dataset you've chosen (e.g., number of features, number of classes)."
      ],
      "metadata": {
        "id": "-30Oxtt_rx4D"
      },
      "id": "-30Oxtt_rx4D"
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose one dataset:\n",
        "# data = load_iris()\n",
        "# data = load_wine()\n",
        "# data = load_breast_cancer() # Recommended for a slightly more complex binary classification\n",
        "\n",
        "X =\n",
        "y =\n",
        "\n",
        "# Display shape of features and labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Number of classes: {np.unique(y).shape[0]}\")\n",
        "print(f\"Feature names: {data.feature_names[:5]}...\") # Show first 5 feature names\n",
        "print(f\"Target names: {data.target_names}\")"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "outputs": [],
      "id": "load_dataset",
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify unique output classes.\n",
        "\n",
        "# Understand the available features as well as target labels."
      ],
      "metadata": {
        "id": "c-wsAvnFsY2d"
      },
      "id": "c-wsAvnFsY2d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Self-Correction/Extension:** If you're feeling adventurous, try loading a dataset from Keras directly (e.g., `keras.datasets.mnist.load_data()`). Be aware that Keras datasets often come pre-split.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "t2ljrxW1rx4E"
      },
      "id": "t2ljrxW1rx4E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Data Preprocessing and Splitting\n",
        "\n",
        "**2.1 Data Splitting (Train, Validation, Test)**\n",
        "\n",
        "* **Task:** Split your dataset into training, validation, and testing sets. This is crucial for evaluating your model's generalization performance.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Use `train_test_split` from `sklearn.model_selection`.\n",
        "\n",
        "    * First, split `X` and `y` into `X_train`, `X_test`, `y_train`, `y_test` with a `test_size` of 20-30% and `random_state` for reproducibility.\n",
        "\n",
        "    * Then, further split `X_train` and `y_train` into `X_train_split`, `X_val`, `y_train_split`, `y_val` (e.g., 20% of the training data for validation).\n",
        "\n",
        "    * Print the shapes of all resulting arrays (`X_train_split`, `X_val`, `X_test`, etc.)."
      ],
      "metadata": {
        "id": "67FBHsearx4E"
      },
      "id": "67FBHsearx4E"
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and testing sets\n",
        "\n",
        "# Using stratify=y ensures that the proportion of target values is the same in both the training and test sets.\n",
        "\n",
        "# Split training set into training and validation sets\n",
        "\n",
        "\n",
        "# display train and test dataset shape\n"
      ],
      "metadata": {
        "id": "data_splitting"
      },
      "outputs": [],
      "id": "data_splitting",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Discussion Point:** Why is it important to have separate training, validation, and test sets? What happens if we only use a train-test split?\n",
        "\n",
        "**2.2 Feature Scaling**\n",
        "\n",
        "* **Task:** Standardize your features. This is a common and often crucial preprocessing step for neural networks, as it helps optimizers converge faster and more effectively.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Initialize a `StandardScaler` from `sklearn.preprocessing`.\n",
        "\n",
        "    * Fit the scaler *only* on your `X_train` data.\n",
        "\n",
        "    * Transform `X_train`, `X_val`, and `X_test` using the fitted scaler.\n",
        "\n",
        "    * Print the mean and standard deviation of a few features in `X_train` *after* scaling to verify."
      ],
      "metadata": {
        "id": "UvOtAXPbrx4E"
      },
      "id": "UvOtAXPbrx4E"
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nMean of X_train_scaled (first 5 features):\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nStandard deviation of X_train_scaled (first 5 features):\")\n"
      ],
      "metadata": {
        "id": "feature_scaling"
      },
      "outputs": [],
      "id": "feature_scaling",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Discussion Point:** Why do we fit the scaler *only* on the training data and then transform all sets? What would happen if we fitted on the entire dataset before splitting?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3lvlGiy1rx4E"
      },
      "id": "3lvlGiy1rx4E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Building and Training Your First Model\n",
        "\n",
        "**3.1 Create a Simple Neural Network Model**\n",
        "\n",
        "* **Task:** Construct a basic feed-forward neural network using Keras's Sequential API.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Define a `Sequential` model.\n",
        "\n",
        "    * Add a few `Dense` layers. The first `Dense` layer needs an `input_shape` argument (based on the number of features in your dataset).\n",
        "\n",
        "    * Use appropriate activation functions (e.g., 'relu' for hidden layers, 'sigmoid' for binary classification output, 'softmax' for multi-class classification output).\n",
        "\n",
        "    * For binary classification (like `load_breast_cancer`), the output layer should have 1 neuron with a 'sigmoid' activation. For multi-class (like `load_iris` or `load_wine`), the output layer should have `num_classes` neurons with 'softmax' activation.\n",
        "\n",
        "    * Print a `model.summary()` to see the architecture and number of parameters.\n",
        "\n",
        "* **Additional Information**\n",
        "    1. Dropout Layer\n",
        "\n",
        "      Purpose: Dropout is a regularization technique used to prevent overfitting. During training, it randomly sets a fraction of input units to 0 at each update step, which helps prevent complex co-adaptations on training data.\n",
        "        \n",
        "        Syntax: `layers.Dropout(rate)`\n",
        "                rate: The fraction of the input units to drop (between 0 and 1). A common value is 0.2 to 0.5.\n",
        "    2. Batch Normalization Layer\n",
        "\n",
        "      Purpose: Batch Normalization normalizes the activations of the previous layer. It helps stabilize and speed up the training process, especially for deeper networks, by reducing internal covariate shift.\n",
        "        \n",
        "        Syntax: `layers.BatchNormalization()`"
      ],
      "metadata": {
        "id": "mhCLdYW0rx4F"
      },
      "id": "mhCLdYW0rx4F"
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of features for the input layer\n",
        "# fill below stamanet\n",
        "input_dim =\n",
        "num_classes = np.unique(y).shape[0] # Number of unique classes\n",
        "\n",
        "# create your own model. Below is sample only. adopt it as per your requirements.\n",
        "model = Sequential([\n",
        "    layers.Dense(32, activation='relu', input_shape=(input_dim,)),\n",
        "    layers.Dense(1, activation='sigmoid') # For binary classification\n",
        "    # For multi-class classification (e.g., Iris, Wine):\n",
        "    # layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "create_model"
      },
      "outputs": [],
      "id": "create_model",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Discussion Point:** What do the 'None' dimensions in `model.summary()` represent? What is the role of activation functions in a neural network?\n",
        "\n",
        "**3.2 Compile the Model**\n",
        "\n",
        "* **Task:** Configure the learning process by specifying the optimizer, loss function, and metrics.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Use `model.compile()`.\n",
        "\n",
        "    * For binary classification, use `loss='binary_crossentropy'`. For multi-class, use `loss='sparse_categorical_crossentropy'` (if your labels are integers) or `loss='categorical_crossentropy'` (if your labels are one-hot encoded).\n",
        "\n",
        "    * Start with the `Adam` optimizer.\n",
        "\n",
        "    * Include `metrics=['accuracy']`."
      ],
      "metadata": {
        "id": "Y-SinAuKrx4F"
      },
      "id": "Y-SinAuKrx4F"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n"
      ],
      "metadata": {
        "id": "compile_model"
      },
      "outputs": [],
      "id": "compile_model",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Discussion Point:** What is the difference between a loss function and a metric? Why do we need an optimizer?\n",
        "\n",
        "**3.3 Train the Model**\n",
        "\n",
        "* **Task:** Train your neural network using the training and validation data.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Use `model.fit()`.\n",
        "\n",
        "    * Pass `X_train_scaled`, `y_train` as training data.\n",
        "\n",
        "    * Pass `X_val_scaled`, `y_val` as validation data (using the `validation_data` argument).\n",
        "\n",
        "    * Set `epochs` (e.g., 20-50) and `batch_size` (e.g., 32).\n",
        "\n",
        "    * Store the training history object in a variable (e.g., `history`)."
      ],
      "metadata": {
        "id": "Je1Mo4Z_rx4F"
      },
      "id": "Je1Mo4Z_rx4F"
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model with 50 epochs and store output to draw the plots for accuracy and loss.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "train_model"
      },
      "outputs": [],
      "id": "train_model",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Discussion Point:** What are epochs and batch size? How do they affect the training process? What does it mean if the validation loss starts increasing while training loss decreases?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "RSe2hd9-rx4F"
      },
      "id": "RSe2hd9-rx4F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Visualization and Experimentation\n",
        "\n",
        "**4.1 Visualize Training History**\n",
        "\n",
        "* **Task:** Plot the training and validation loss, and training and validation accuracy over epochs. This helps in understanding model performance and identifying overfitting/underfitting.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Access the `history.history` dictionary.\n",
        "\n",
        "    * Plot 'accuracy' vs. 'val_accuracy' and 'loss' vs. 'val_loss' using `matplotlib.pyplot`.\n",
        "\n",
        "    * Add titles, labels, and legends to your plots."
      ],
      "metadata": {
        "id": "M8ZKGswPrx4F"
      },
      "id": "M8ZKGswPrx4F"
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust below code to draw the plots for accuracy and loss\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "visualize_history"
      },
      "outputs": [],
      "id": "visualize_history",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Analysis:** Based on the plots, is your model overfitting, underfitting, or performing well? How can you tell?\n",
        "\n",
        "**4.2 Experiment with Different Optimizers**\n",
        "\n",
        "* **Task:** Re-train your model using different optimizers (e.g., `SGD`, `RMSprop`) and compare their performance.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Create a *new* model (or re-initialize your existing one to ensure a clean slate).\n",
        "\n",
        "    * Compile the new model, but this time use `optimizer=SGD()` (with or without learning rate, e.g., `SGD(learning_rate=0.01)`).\n",
        "\n",
        "    * Train the model for the same number of epochs and batch size.\n",
        "\n",
        "    * Repeat the process for `optimizer=RMSprop()`.\n",
        "\n",
        "    * Compare the plots of accuracy and loss for each optimizer."
      ],
      "metadata": {
        "id": "0cQ9lNR6rx4F"
      },
      "id": "0cQ9lNR6rx4F"
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with optimizer SGD with learning_rates 0.01, 0.1, 0.2\n",
        "print(\"\\n--- Training with SGD Optimizer ---\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Experiment with RMSprop with learning_rates 0.01, 0.1, 0.2\n",
        "print(\"\\n--- Training with RMSprop Optimizer ---\")\n",
        "\n",
        "\n",
        "# Adjust below code for comparing the models (you'll need to adapt the plotting code from 4.1)\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['val_accuracy'], label='Adam')\n",
        "plt.plot(history_sgd.history['val_accuracy'], label='SGD')\n",
        "plt.plot(history_rmsprop.history['val_accuracy'], label='RMSprop')\n",
        "plt.title('Validation Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['val_loss'], label='Adam')\n",
        "plt.plot(history_sgd.history['val_loss'], label='SGD')\n",
        "plt.plot(history_rmsprop.history['val_loss'], label='RMSprop')\n",
        "plt.title('Validation Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['accuracy'], label='Adam')\n",
        "plt.plot(history_sgd.history['accuracy'], label='SGD')\n",
        "plt.plot(history_rmsprop.history['accuracy'], label='RMSprop')\n",
        "plt.title('Training Accuracy Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "experiment_optimizers"
      },
      "outputs": [],
      "id": "experiment_optimizers",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Discussion Point:** Which optimizer performed best for your dataset? Why do you think some optimizers perform better than others in certain situations?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Bug_E7RIrx4G"
      },
      "id": "Bug_E7RIrx4G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5: Model Evaluation on Test Set\n",
        "\n",
        "**5.1 Evaluate Final Model**\n",
        "\n",
        "* **Task:** Evaluate the performance of your *best-performing* model (from your optimizer experiments) on the unseen test set.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Use `model.evaluate()` (for your best model) on `X_test_scaled` and `y_test`.\n",
        "\n",
        "    * Print the test loss and test accuracy."
      ],
      "metadata": {
        "id": "BRWamM70rx4G"
      },
      "id": "BRWamM70rx4G"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'model' (trained with Adam) was your best performing model\n",
        "# Or choose model_sgd or model_rmsprop if they performed better\n",
        "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "evaluate_model"
      },
      "outputs": [],
      "id": "evaluate_model",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Discussion Point:** How does the test accuracy compare to the validation accuracy? What does this tell you about your model's generalization?\n",
        "\n",
        "**5.2 Make Predictions and Analyze Metrics**\n",
        "\n",
        "* **Task:** Make predictions on the test set and calculate additional classification metrics.\n",
        "\n",
        "* **Instructions:**\n",
        "\n",
        "    * Use `model.predict()` on `X_test_scaled` to get raw predictions.\n",
        "\n",
        "    * Convert probabilities to binary class predictions (e.g., for sigmoid output, values > 0.5 are class 1, otherwise class 0).\n",
        "\n",
        "    * Calculate and print the `accuracy_score`, `confusion_matrix`, and `classification_report` from `sklearn.metrics`."
      ],
      "metadata": {
        "id": "sBrsHI8Brx4G"
      },
      "id": "sBrsHI8Brx4G"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probs = model.predict(X_test_scaled)\n",
        "# For binary classification with sigmoid output:\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# For multi-class classification with softmax output:\n",
        "# y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "print(\"\\n--- Classification Metrics on Test Set ---\")\n",
        "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "id": "analyze_metrics"
      },
      "outputs": [],
      "id": "analyze_metrics",
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Discussion Point:** What do precision, recall, and F1-score tell you about your model's performance beyond just accuracy? When would each metric be more important?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jfbu3h81rx4G"
      },
      "id": "jfbu3h81rx4G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge & Extension Activities (If Time Permits)\n",
        "\n",
        "* **Experiment with Model Architecture:**\n",
        "\n",
        "    * Add more layers or change the number of neurons in existing layers.\n",
        "\n",
        "    * Try different activation functions (e.g., `tanh`, `sigmoid` for hidden layers, though `relu` is often a good default).\n",
        "\n",
        "* **Hyperparameter Tuning:**\n",
        "\n",
        "    * Adjust the `learning_rate` for your optimizers.\n",
        "\n",
        "    * Change the `batch_size` and `epochs`.\n",
        "\n",
        "* **Regularization:** Introduce `Dropout` layers to combat overfitting.\n",
        "\n",
        "* **Different Datasets:** Repeat the process with a different dataset from `sklearn.datasets` (e.g., `load_diabetes` for regression, though you'd need to change the output layer, loss function, and metrics).\n",
        "\n",
        "* **K-Fold Cross-Validation:** Explain how scikit-learn's `KFold` could be used to get a more robust estimate of model performance, especially with smaller datasets. (This is more conceptual for an in-class assignment but good for discussion).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "d7StNA6yrx4G"
      },
      "id": "d7StNA6yrx4G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deliverables:\n",
        "\n",
        "Students should submit their Jupyter Notebook containing all the code, outputs, and answers to the discussion questions. Encourage them to add comments to their code to explain each step."
      ],
      "metadata": {
        "id": "JF3ZHU7wrx4G"
      },
      "id": "JF3ZHU7wrx4G"
    }
  ]
}